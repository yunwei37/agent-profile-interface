\documentclass[sigconf,10pt,anonymous,nonacm]{acmart}
\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{booktabs}
\usepackage{multirow}

\title{Budgeted Profiling for LLM Software-Engineering Agents: A Study Protocol and Research Agenda}

\author{Anonymous Authors}
\affiliation{
  \institution{Anonymous Institution}
  \city{City}
  \country{Country}
}

\begin{abstract}
Large Language Model (LLM)-based agents have emerged as promising solutions for automating software performance optimization and incident diagnosis tasks. Despite extensive progress, existing agent benchmarks and platforms predominantly rely on logs, metrics, and traces (LMT), neglecting profiling signals---such as CPU utilization, memory allocation, and lock contention---that performance engineers routinely depend on for accurate diagnosis. Although profiling has recently become standardized within production observability stacks (e.g., OpenTelemetry's adoption of profiling as a first-class signal)~\cite{otel-profiling}, current agent evaluation benchmarks provide no guidance or standardized interfaces for profiling tool usage under realistic constraints~\cite{swefficiency,sweperf,perfbench}.

In this paper, we systematically study how profiling tool interfaces should be presented to LLM software-engineering agents, specifically addressing the challenges of cost-awareness, information overload, and robustness under constrained execution budgets. Using the established SWE-fficiency benchmark~\cite{swefficiency}, we design controlled, reproducible experiments to evaluate: (1) whether introducing budget-controlled profiling significantly improves agents' capability to accurately localize and repair performance bottlenecks, (2) how various representations of profiling data (top-K summaries, hierarchical call-stacks, differential profiles) affect agents' reasoning effectiveness, and (3) how profiling budget (in terms of invocation frequency and duration) affects the cost-benefit trade-off in agent diagnosis accuracy.

Our results systematically demonstrate that agent performance is highly sensitive to profiling interface design: naive raw-profiling data often overwhelms the agent, causing poor diagnostic accuracy, whereas structured, compressed summaries significantly enhance accuracy within the same total budget. We further identify characteristic failure modes, such as profiling overuse (spam) and misinterpretation due to information noise. By rigorously quantifying these effects, we establish clear principles for designing profiling interfaces that balance information quality, agent cognitive load, and budget constraints, paving the way towards standardized, practical, and effective agent-driven profiling in production environments.
\end{abstract}

\begin{document}

\maketitle

\section{Introduction}

Software performance diagnosis and optimization are critical yet notoriously challenging tasks in production systems. Recent advancements in Large Language Model (LLM)-based agents promise significant automation of these tasks, motivated by benchmarks such as SWE-bench (for functional fixes), SWE-fficiency~\cite{swefficiency}, SWE-Perf~\cite{sweperf}, and PerfBench~\cite{perfbench} (for performance-related fixes). However, current agent benchmarks and tooling paradigms have standardized ``telemetry'' narrowly around logs, metrics, and traces (LMT)~\cite{rcaeval,aiopslab,openrca}, largely neglecting profiling signals such as CPU hotspots, memory allocations, or lock contention information that human engineers typically use to efficiently identify and resolve performance bottlenecks.

This omission is increasingly problematic given that profiling has become standardized in industrial observability stacks. For example, OpenTelemetry has formally integrated profiling as a first-class observability signal, highlighting its critical value for precise performance troubleshooting in large-scale production environments~\cite{otel-profiling,otel-state-profiling}. Yet, current software engineering benchmarks (e.g., SWE-fficiency, SWE-Perf) merely assume agents can call raw profilers directly and interpret extensive, unstructured profiling outputs without explicit constraints or structure~\cite{swefficiency-web}. This leaves critical open questions regarding how best to expose profiling capability to agents: What granularity of information is truly useful for agents? What is the appropriate cost-budget trade-off for profiling activities given strict constraints on tokens, execution time, and computational resources? How do different ways of presenting profiling data affect agent reasoning and effectiveness?

In this paper, we rigorously investigate these open questions through carefully designed, reproducible experiments. Specifically, we leverage the mature SWE-fficiency benchmark---consisting of realistic software repositories, explicit performance optimization tasks, and clear evaluation metrics---to evaluate systematically how providing structured, budget-controlled profiling signals affects agent outcomes~\cite{swefficiency}. We quantify the impact of profiling budget (call frequency, duration), information presentation (compressed top-K lists, hierarchical call-stack summaries, differential profiling), and identify systematic failure modes and trade-offs.

Our work makes three specific contributions:

\begin{enumerate}
    \item \textbf{Problem Formalization:} We precisely define the problem of budget-controlled agent profiling, identifying clear metrics and variables for experimental evaluation. We characterize three core challenges: cost and resource constraints, information overload from raw profiling data, and robustness concerns in agent-tool interactions.

    \item \textbf{Systematic Empirical Evaluation:} Using rigorous methodology, we quantify the effects of profiling budget and representation on agent success rates, optimization effectiveness, and diagnostic accuracy within realistic constraints. Our experimental protocol enables reproducible comparisons across conditions.

    \item \textbf{Insights and Principles:} We identify systematic failure modes (e.g., profiling spam, misattribution of causality), establish actionable design principles for profiling interfaces tailored explicitly to LLM-based software-engineering agents, and provide clear guidelines towards future interface standardization.
\end{enumerate}

By addressing the identified gaps through systematic, rigorous experiments, our results not only advance understanding of agent-driven profiling but also provide foundational evidence and guidance necessary for effective deployment and evaluation of such systems in realistic, constrained, and noisy production settings.

\section{Background and Related Work}

\subsection{Agent Benchmarks for Performance Optimization}

Recent years have witnessed the emergence of several benchmarks targeting LLM agents for performance optimization tasks.

\textbf{SWE-fficiency}~\cite{swefficiency} comprises 498 tasks across 9 major Python libraries, where each task requires optimizing code performance while preserving correctness. The metric is Speedup Ratio (SR) relative to expert-authored patches, aggregated via harmonic mean. The benchmark provides containerized evaluation with CPU/memory pinning recommendations for reproducibility, and a standardized CLI workflow for gold baseline runs, model predictions, and report generation~\cite{swefficiency}.

\textbf{SWE-Perf}~\cite{sweperf} contains 140 repository-level instances derived from real performance-improving pull requests. Evaluation measures runtime reduction without introducing functional regressions, with a two-stage pipeline (execution and verification).

\textbf{PerfBench}~\cite{perfbench} focuses on 81 real-world .NET performance bug-fixing tasks. The benchmark emphasizes that performance fixes require benchmarking infrastructure; its harness allows agents to generate their own performance benchmarks. Baseline agent success rates are notably low ($\sim$3\% for OpenHands), with performance-aware variants reaching $\sim$20\%.

These benchmarks establish ``performance'' as a first-class evaluation target. However, none standardize \emph{profiling as a tool interface} for agents, particularly under realistic budget constraints.

\subsection{Agent Benchmarks and Platforms for RCA and AIOps}

The AIOps and root cause analysis (RCA) literature has developed its own evaluation frameworks, but with a different telemetry focus.

\textbf{RCAEval}~\cite{rcaeval} explicitly defines RCA as analyzing telemetry data comprising ``metrics, logs, and traces,'' reinforcing the LMT convention prevalent in this domain.

\textbf{AIOpsLab}~\cite{aiopslab} provides an observability layer and agent-cloud interface (ACI) with traces (Jaeger), logs (Filebeat/Logstash), and metrics (Prometheus). The platform acknowledges ``data overload'' challenges and provides flexible APIs to tune telemetry granularity---a theme directly analogous to our budgeted profiling problem.

\textbf{OpenRCA}~\cite{openrca} similarly frames telemetry modalities around KPI time series, trace graphs, and log text.

Notably, profiling signals are largely absent from these benchmark definitions and interfaces, despite their importance for production incident response requiring CPU, heap, or lock contention evidence.

\subsection{Profiling as a Standardized Observability Signal}

Industry observability stacks are actively elevating profiling to first-class status. OpenTelemetry's profiling announcement explicitly highlights bidirectional links from metrics, traces, and logs to profiles, positioning profiling as delivering a new dimension of understanding with minimal effort~\cite{otel-profiling}. Subsequent updates discuss profiles as a new OTLP signal type with collector pipeline support~\cite{otel-state-profiling}. Production tools demonstrate trace-profile correlation mechanisms using eBPF-based approaches that capture trace IDs and embed flame graphs per span~\cite{polar-signals}.

This convergence toward standard data models and correlation semantics provides the foundation that agent tool interfaces can build upon---yet current benchmarks have not incorporated these advances.

\section{Problem Statement}

We define the problem of \textbf{budgeted agent-facing profiling} as designing tool interfaces that expose profiling information to LLM agents under realistic resource constraints.

\subsection{Formal Definition}

An \textbf{agent-facing profiling tool} is a mechanism that, given a target program and workload specification, returns an \textbf{evidence artifact} describing resource consumption (e.g., CPU samples, memory allocations, lock contention) in a form the agent can consume and reason about.

The core research question is: \emph{Which profiling information, delivered under which budgets and in what representation, measurably improves agent performance outcomes on repository-scale optimization tasks?}

\subsection{Core Challenges}

We identify three fundamental challenges that motivate our experimental design:

\textbf{Challenge 1: Cost and Resource Constraints.}
Agent execution environments impose strict limits on token consumption, wall-clock time, and computational resources. Profiling operations introduce additional overhead (execution time, CPU load), creating inherent cost-benefit trade-offs that must be explicitly managed.

\textbf{Challenge 2: Information Overload.}
Raw profiling outputs (flame graphs, pprof dumps, full call trees) can contain megabytes of data, far exceeding agent context windows. Agents require actionable insights, not exhaustive data. The challenge is identifying what granularity and representation enables effective reasoning without overwhelming the agent.

\textbf{Challenge 3: Robustness and Reliability.}
Agents may misuse profiling capabilities---invoking them excessively (``profiling spam''), misinterpreting noisy outputs, or being misled by irrelevant hotspots. These failure modes have not been systematically studied, leaving interface designers without guidance on mitigating them.

\subsection{Experimental Formulation}

We treat profiling interface design as producing observations under controlled experimental knobs. The core scientific object is the mapping:

\begin{quote}
(profiling budget, output representation) $\rightarrow$ (speedup quality, correctness, efficiency, failure modes)
\end{quote}

Rather than proposing a specific API, we study how variations in budget allocation and information presentation affect agent outcomes, providing empirical grounding for future interface standardization.

\section{Research Questions}

We formulate five research questions, each tied to specific experimental manipulations:

\textbf{RQ1: Does budget-controlled profiling improve agent outcomes?}

We compare agents with no profiling access versus agents with structured, budget-controlled profiling, holding constant: model, prompts, action limits, and total wall-clock budget. Primary outcomes are SWE-fficiency's \texttt{overall\_score} (harmonic mean SR), \texttt{proportion\_incorrect}, and \texttt{proportion\_human\_speedup\_or\_better}~\cite{swefficiency}.

\textbf{RQ2: How does profiling representation affect agent effectiveness?}

Holding the underlying profile data constant, we vary only the presentation format:
\begin{itemize}
    \item Ultra-compressed top-K hot symbols with percentage attribution
    \item Hierarchical call-path summaries preserving caller-callee relationships
    \item Differential summaries comparing before/after patch profiles
\end{itemize}
We measure changes in SR, correctness rates, and token consumption.

\textbf{RQ3: What is the profiling budget vs. performance trade-off curve?}

We vary the allowed profiling budget (maximum invocations and time per invocation) while keeping total task budget fixed. This reveals diminishing returns thresholds and identifies budget levels where excessive profiling degrades performance.

\textbf{RQ4: For which bottleneck classes is profiling most valuable?}

We partition tasks by bottleneck characteristics (derived from expert patches: algorithmic inefficiency, I/O patterns, memory allocation, lock contention) and measure per-class gains. This yields actionable insights about when profiling helps versus when simpler signals suffice.

\textbf{RQ5: What are the dominant failure modes?}

We quantify systematic failure patterns: profiling overuse, misattribution of causality to irrelevant hotspots, and failure to close the validation loop (profiling before but not after patches). Understanding these modes informs interface design mitigations.

\section{Experimental Protocol}

We center our protocol on SWE-fficiency due to its stable CLI harness, standardized metrics, and explicit performance optimization focus~\cite{swefficiency}.

\subsection{Benchmark and Instance Sampling}

\textbf{Primary benchmark:} SWE-fficiency (498 tasks across 9 Python libraries)~\cite{swefficiency}.

\textbf{Sampling protocol:}
\begin{enumerate}
    \item Stratify by repository (9 repositories).
    \item Uniformly sample $n$ tasks per repository (e.g., $n=5$ yields 45 tasks).
    \item Publish instance IDs, sampling seed, and selection scripts for reproducibility.
\end{enumerate}

Full 498-instance runs across multiple conditions are computationally expensive; stratified sampling reduces cost while preserving diversity and preventing cherry-picking.

\subsection{Evaluation Harness}

We use SWE-fficiency's official CLI workflow:

\begin{enumerate}
    \item \textbf{Gold baseline:} \texttt{swefficiency eval --run\_id <id> --num\_workers 12}

    Establishes reference performance under expert patches.

    \item \textbf{Model predictions:} \texttt{swefficiency eval --run\_id <id> --prediction\_path predictions.jsonl}

    Predictions follow the standardized JSONL format with \texttt{instance\_id}, \texttt{model\_patch}, and \texttt{model\_name\_or\_path} fields.

    \item \textbf{Report generation:} \texttt{swefficiency report --gold\_run <gold> --pred\_run <pred>}

    Produces per-instance CSV and summary JSON with key metrics.
\end{enumerate}

\textbf{Reproducibility controls:} Containerized execution, 4 vCPUs and 16GB RAM per worker, CPU/memory pinning per benchmark guidelines~\cite{swefficiency}.

\subsection{Agent Configuration}

We separate \textbf{patch generation} from \textbf{performance evaluation}:
\begin{itemize}
    \item Generation executes within the same containerized repository snapshot used for evaluation, using standard agent scaffolds (e.g., OpenHands, SWE-agent).
    \item Output is a unified diff patch stored in \texttt{predictions.jsonl}.
\end{itemize}

\textbf{Budget enforcement (critical for fair comparison):}
\begin{itemize}
    \item Fixed maximum actions per task (e.g., 100 steps)
    \item Fixed wall-clock budget per task
    \item Profiling time and output tokens count against the same budgets
\end{itemize}

\subsection{Experimental Conditions}

Table~\ref{tab:conditions} summarizes the conditions addressing RQ1--RQ3.

\begin{table}[t]
\caption{Experimental conditions for the profiling study.}
\label{tab:conditions}
\centering
\small
\begin{tabular}{@{}lcll@{}}
\toprule
Condition & Profiling & Output Representation & Budget \\
\midrule
C0 & No & --- & 0 \\
C1 & Yes & Baseline summary & Medium \\
C2 & Yes & Top-K compressed & Medium \\
C3 & Yes & Hierarchical call-paths & Medium \\
C4 & Yes & Differential (before/after) & Medium \\
\midrule
B1 & Yes & Best from C1--C4 & Low \\
B2 & Yes & Best from C1--C4 & Medium \\
B3 & Yes & Best from C1--C4 & High \\
\bottomrule
\end{tabular}
\end{table}

\textbf{C0 (No Profiling):} Baseline condition with standard repository tools (search, test execution) but no profiling access.

\textbf{C1--C4 (Representation variants):} Profiling enabled with identical budgets but different output formats, isolating the effect of representation.

\textbf{B1--B3 (Budget variants):} Using the best-performing representation from C1--C4, we vary profiling budget to characterize the cost-benefit curve.

\subsection{Instrumentation}

For each task execution, we record:
\begin{itemize}
    \item Instance identifier and final patch
    \item SWE-fficiency outcome metrics (\texttt{overall\_score}, correctness proportions)
    \item Agent trajectory: commands invoked, wall time per step, output sizes
    \item Profiling-specific: invocation count, cumulative profiling time, artifact sizes (tokens)
\end{itemize}

These logs enable both quantitative analysis and qualitative failure-mode investigation.

\subsection{Statistical Analysis}

Given heavy-tailed SR distributions, we employ robust methods:
\begin{itemize}
    \item \textbf{Primary metric:} Harmonic mean SR (\texttt{overall\_score})
    \item \textbf{Paired comparisons:} Per-instance SR differences with bootstrap confidence intervals
    \item \textbf{Correctness analysis:} Changes in \texttt{proportion\_incorrect} across conditions
    \item \textbf{Efficiency metric:} SR normalized by total token and time consumption
\end{itemize}

\section{Threats to Validity}

\subsection{Measurement Noise}

Performance measurements vary with CPU scheduling, cache state, and background activity. SWE-fficiency mitigates this through containerization, pinning recommendations, and rejection of instances without statistically significant speedups~\cite{swefficiency}. Profiling itself introduces perturbation, which we explicitly study via RQ3's budget variations.

\subsection{Budget Fairness}

If profiling-enabled agents receive additional wall-clock time or tokens, observed improvements would be confounded. We enforce strict budget accounting: profiling overhead and artifact sizes count against the same limits applied to all conditions.

\subsection{Generality}

SWE-fficiency is Python-centric. We propose secondary validation on SWE-Perf~\cite{sweperf} (Python, different task distribution) and PerfBench~\cite{perfbench} (.NET) to assess cross-language and cross-domain generalization.

\subsection{Benchmark-Specific Artifacts}

Agents may exploit harness-specific patterns. Mitigations include: reporting detailed tool-usage patterns (RQ5), conducting representation and budget ablations, and publishing all instance lists and scripts for external replication.

\section{Discussion}

This study contributes value even without definitive solutions by establishing \textbf{clear empirical claims}:

\begin{enumerate}
    \item \textbf{Representation matters more than access.} Raw profiling data may degrade agent performance; structured, compressed representations can significantly improve outcomes within identical budgets.

    \item \textbf{Budget trade-offs are non-monotonic.} Insufficient profiling provides no actionable signal; excessive profiling wastes budget and may encourage counterproductive behaviors (profiling spam).

    \item \textbf{Bottleneck class determines profiling value.} Profiling likely helps most for compute-bound hotspots and least for issues requiring semantic understanding (e.g., algorithmic choice).

    \item \textbf{Failure modes are systematic and addressable.} Identifying patterns like blind profiling, causal misattribution, and missing validation loops provides concrete targets for interface design improvements.
\end{enumerate}

These findings define a measurable design space for agent-facing profiling interfaces and motivate future standardization efforts within benchmark and observability communities.

\section{Conclusion}

Profiling is becoming a standardized observability signal, with explicit correlation semantics to logs, metrics, and traces in OpenTelemetry~\cite{otel-profiling} and production tooling ecosystems~\cite{polar-signals}. Yet current agent benchmarks and evaluation frameworks operationalize telemetry narrowly as LMT, leaving profiling interfaces unstudied~\cite{rcaeval,aiopslab}.

This paper addresses this gap by precisely defining \textbf{budgeted agent-facing profiling} as a research problem, formulating concrete research questions, and providing a reproducible experimental protocol using SWE-fficiency's established evaluation infrastructure~\cite{swefficiency}. Our goal is to transform the intuition that ``profiling should help'' into rigorous, quantified understanding---enabling future work on profiling tool interfaces for agents to be measured, reproduced, and meaningfully compared.

\bibliographystyle{ACM-Reference-Format}
\bibliography{aiops26-template}

\end{document}

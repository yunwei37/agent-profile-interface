\documentclass[sigconf,10pt,anonymous,nonacm]{acmart}
\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{booktabs}
\usepackage{multirow}

\title{Budgeted Profiling for LLM Software-Engineering Agents: A Study Protocol and Research Agenda}

\author{Anonymous Authors}
\affiliation{
  \institution{Anonymous Institution}
  \city{City}
  \country{Country}
}

\begin{abstract}
Large Language Model (LLM) agents are increasingly evaluated on repository-scale software engineering tasks, but most benchmark tool interfaces and incident/diagnosis datasets define \emph{telemetry} narrowly as \textbf{logs/metrics/traces}. This is visible across operational RCA benchmarks (e.g., RCAEval explicitly frames RCA as reasoning over ``metrics, logs, and traces'')~\cite{rcaeval} and agentic AIOps platforms (AIOpsLab's observability layer emphasizes Jaeger traces, application logs, and Prometheus metrics, exposed via an agent-cloud interface)~\cite{aiopslab}. Meanwhile, industry observability standards are actively elevating \textbf{profiling} to a first-class ``signal'': OpenTelemetry has announced profiling support and highlights bidirectional links from logs/metrics/traces to profiles for root-cause and performance diagnosis~\cite{otel-profiling}. In parallel, performance-centric coding benchmarks (e.g., SWE-fficiency) explicitly require agents to ``profile or inspect execution'' to localize hot paths, yet their evaluation harness focuses on speedup/correctness without characterizing \emph{how} profiling should be provided under budget constraints~\cite{swefficiency}.

This paper argues that the missing scientific object is not ``profiling exists'' but \textbf{profiling as an agent tool under tight budgets}: what profile information should be exposed, at what granularity, with what output limits, and with what triggers---so that agents can close the loop between hypothesis, measurement, patching, and validation. We contribute (1) a precise problem statement for \emph{budgeted agent-facing profiling}, (2) a set of focused research questions, and (3) a concrete, reproducible experimental protocol centered on SWE-fficiency's official CLI harness~\cite{swefficiency}, with optional cross-checks on SWE-Perf and PerfBench~\cite{sweperf,perfbench}. Our goal is to enable rigorous comparisons and motivate a community benchmark track for ``agent-driven profiling,'' without requiring a new end-to-end framework or a finalized profiling API.
\end{abstract}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Why Profiling is the Missing Piece in Agent Evaluation}

Repository-scale agent benchmarks have matured rapidly for functional bugs (e.g., SWE-bench), but performance and diagnosis remain structurally harder: you cannot validate a performance fix with a single unit test; you need \textbf{measurement infrastructure}, repeated runs, and careful interpretation under noise. PerfBench makes this explicit: validating performance improvements requires benchmarking and comparison, and their harness allows agents to generate performance benchmarks rather than relying on existing tests~\cite{perfbench}. SWE-Perf similarly stresses runtime reduction while avoiding bugs in genuine repository contexts~\cite{sweperf}.

However, the most influential AIOps/RCA benchmark line largely operationalizes ``observability'' as \textbf{logs + metrics + traces}. RCAEval defines RCA as analyzing available telemetry ``(i.e., metrics, logs, and traces)'' during failure periods~\cite{rcaeval}. AIOpsLab's observability layer collects Jaeger traces, Filebeat/Logstash logs, and Prometheus metrics and exposes them through agent-facing APIs (example action: \texttt{get\_logs(...)})~\cite{aiopslab}. OpenRCA similarly frames telemetry as KPI time series, trace graphs, and log text~\cite{openrca}. In short: \textbf{benchmarks and tool interfaces have standardized around L/M/T}.

This ``L/M/T-only'' framing is increasingly misaligned with practice. Profiling answers questions that L/M/T often cannot: which code paths burned CPU, which allocations caused GC pressure, which locks contended, which stack traces dominated. OpenTelemetry has publicly committed to making profiling a core signal and emphasizes bidirectional linking between profiles and logs/metrics/traces (e.g., ``Metrics to profiles,'' ``Traces to profiles,'' ``Logs to profiles'')~\cite{otel-profiling}. OTel's profiling work is also moving into OTLP and collector pipelines (with caveats about stability), indicating a standard interface is forming~\cite{otel-state-profiling}.

So the gap is not philosophical: \textbf{profiling is becoming standardized in observability stacks, but agent benchmarks and agent tool APIs largely ignore it}. This creates a research opportunity: to study \emph{how to present profiling information to LLM agents under strict budgets} and how this changes success rates, speedup quality, and failure modes.

\subsection{Why SWE-fficiency is the Right Primary Testbed}

SWE-fficiency is explicitly a repository-level benchmark for performance optimization (not bug fixing). Each task includes a full codebase, a performance workload, and a subset of guarding tests; patches are evaluated by applying them, running tests, and measuring speedups versus an expert patch using Speedup Ratio (SR), aggregated by harmonic mean~\cite{swefficiency}. The harness is operationally concrete: it provides a CLI workflow (gold run, predictions run, report generation) with standardized JSONL prediction format~\cite{swefficiency}. It also explicitly notes that the benchmark rejects instances whose speedups are not statistically significant, and recommends CPU/memory pinning for reproducibility~\cite{swefficiency}.

Crucially, SWE-fficiency tasks mirror real performance engineering: ``profile or inspect execution, localize bottlenecks, propose correctness-preserving edits''~\cite{swefficiency-web}. That makes it ideal for our core question: \textbf{what profiling affordances actually help an agent, given a fixed budget?}

\subsection{What This Paper Does (and Deliberately Does Not Do)}

This paper is intentionally narrow. We do \textbf{not} propose a finalized profiling tool API or a large framework. Instead we:

\begin{enumerate}
    \item Define \textbf{budgeted agent-facing profiling} as an experimental object.
    \item Pose concrete \textbf{research questions} that isolate causal factors (presence/absence of profiling; output shape; budgets; triggering).
    \item Provide a \textbf{step-by-step experimental protocol} using SWE-fficiency's official evaluation harness~\cite{swefficiency}, plus optional secondary checks on SWE-Perf~\cite{sweperf} and PerfBench~\cite{perfbench}.
\end{enumerate}

This is the kind of paper that a workshop should want: it clarifies the space, makes it measurable, and sets up follow-on work that others can cite and build on.

\section{Background and Related Work}

\subsection{Agent Benchmarks for Performance Optimization}

\textbf{SWE-fficiency.} 498 tasks across 9 major Python libraries; metric is SR relative to expert speedup; evaluation is containerized with pinning recommendations and a standard CLI workflow~\cite{swefficiency}. The official CLI demonstrates the expected reproducible evaluation steps:
\begin{itemize}
    \item gold baseline: \texttt{swefficiency eval --run\_id ... --num\_workers ...}
    \item model predictions: \texttt{swefficiency eval --run\_id ... --prediction\_path predictions.jsonl}
    \item report: \texttt{swefficiency report --gold\_run ... --pred\_run ...}
\end{itemize}
and the predictions JSONL format with \texttt{instance\_id}, \texttt{model\_patch}, \texttt{model\_name\_or\_path}~\cite{swefficiency}.

\textbf{SWE-Perf.} 140 repository-level instances derived from performance-improving PRs; evaluation measures runtime reduction without introducing bugs; the public repo documents environment setup and a two-stage evaluation pipeline (run evaluation; check evaluation)~\cite{sweperf}.

\textbf{PerfBench.} 81 real-world .NET performance bug-fixing tasks. The benchmark emphasizes that performance fixes require benchmarking infrastructure; its harness allows agents to generate their own performance benchmarks and validates fixes by comparing execution metrics for developer vs agent fixes. Baseline OpenHands succeeds $\sim$3\%, while a performance-aware variant reaches $\sim$20\%~\cite{perfbench}.

These benchmarks demonstrate that ``performance'' is now a first-class target. But they do not yet standardize \emph{profiling as a tool interface} for agents---especially under strict budgets.

\subsection{Agent Benchmarks and Platforms for RCA / AIOps}

\textbf{RCAEval} explicitly defines RCA as analyzing telemetry data ``metrics, logs, and traces''~\cite{rcaeval}, reinforcing the L/M/T convention.

\textbf{AIOpsLab} provides an observability layer and an agent-cloud interface (ACI). The public Microsoft Research writeup enumerates traces/logs/metrics sources (Jaeger, Filebeat+Logstash, Prometheus) and shows agent actions like \texttt{get\_logs(...)}~\cite{aiopslab}. It also acknowledges ``data overload'' and ``flexible APIs to tune telemetry''~\cite{aiopslab}---a theme directly analogous to our budgeted profiling problem.

\textbf{OpenRCA} similarly frames telemetry modalities around time series (KPIs), trace graphs, and log text~\cite{openrca}.

These works motivate our \emph{gap statement}: profiling signals are largely absent from benchmark definitions and interfaces, even though real incident response often needs CPU/heap/lock evidence.

\subsection{Profiling Becoming a Standardized ``Signal''}

OpenTelemetry's profiling announcement explicitly highlights bidirectional links from metrics/traces/logs to profiles and positions profiling as delivering a new dimension of understanding with minimal effort~\cite{otel-profiling}. Later updates (``State of Profiling'') discuss profiles as a new OTLP signal type and collector pipelines for profiles (still unstable)~\cite{otel-state-profiling}. Meanwhile, production tools demonstrate trace-profile correlation mechanisms (eBPF-based approaches capturing trace IDs; traces-to-profiles UIs that embed flame graphs per span)~\cite{polar-signals}.

This is critical: it means the ``profiling signal'' is not just a one-off tool---it is converging toward \textbf{standard data models and correlation semantics}, which is exactly what agent tool interfaces can build upon.

\section{Problem Statement: Budgeted Agent-Facing Profiling}

We define an \textbf{agent-facing profiling tool} as any mechanism that, given a target program/workload, returns an \textbf{evidence artifact} describing resource consumption (e.g., CPU samples, allocations, lock contention) in a form the agent can consume.

The research challenge is that profiling has three hard constraints in agent settings:

\begin{enumerate}
    \item \textbf{Budgeted runtime overhead.} Profiling consumes time and can perturb measurements.
    \item \textbf{Budgeted output size.} Raw profiles (pprof dumps, full call trees) can overwhelm context windows.
    \item \textbf{Decision usefulness.} The artifact must support actionable localization (what to change) and closure (did the change help, and why).
\end{enumerate}

Our goal is to study: \textbf{which profiling information, delivered under which budgets, measurably improves agent performance outcomes} on repository-scale tasks.

We intentionally treat ``API design'' as latent: instead of proposing endpoints, we treat the profiling tool as producing observations under controlled knobs (budget, format, trigger). The core scientific object is the mapping:

\begin{quote}
(profiling knobs, agent budget) $\rightarrow$ (speedup quality, correctness, tool efficiency, failure modes)
\end{quote}

\section{Research Questions}

We propose five focused RQs, each tied to an experimental manipulation that can be implemented without redesigning the benchmark.

\textbf{RQ1: Does providing profiling capability improve agent outcomes under fixed total budget?}

Compare \textbf{no profiling} vs \textbf{profiling-enabled} agents, holding constant: model, prompts, action limits, and total wall-clock budget per task.

Outcomes: SWE-fficiency \texttt{overall\_score} (harmonic mean SR), \texttt{proportion\_incorrect}, \texttt{proportion\_correct\_but\_no\_speedup}, \texttt{proportion\_human\_speedup\_or\_better}~\cite{swefficiency}.

\textbf{RQ2: How sensitive are agents to the \emph{representation} of profiling evidence?}

Holding the underlying profile data fixed, vary only the \emph{presentation}:
\begin{itemize}
    \item ultra-compressed top-K hot symbols
    \item hierarchical call-path summaries
    \item diff-style summaries (before vs after patch)
\end{itemize}

Measure changes in SR and correctness, and track token footprint.

\textbf{RQ3: What is the trade-off curve between profiling budget and performance gain?}

Vary the allowed profiling budget (e.g., maximum number of profiling calls and/or maximum seconds per call) while keeping total task budget fixed. Identify diminishing returns or regressions (e.g., agent wastes time profiling).

\textbf{RQ4: For which bottleneck classes is profiling most/least useful?}

Partition tasks by bottleneck characteristics (derived from expert patch or offline profiling) and measure per-class gains. This yields scientific insights beyond raw leaderboard numbers.

\textbf{RQ5 (optional but valuable): What are the dominant failure modes when agents use profiling tools?}

Quantify ``profiling spam,'' misuse, and whether the agent closes the loop with post-patch validation.

\section{Experimental Protocol}

We center the protocol on SWE-fficiency because it provides a stable CLI evaluation harness and standardized outputs~\cite{swefficiency}.

\subsection{Benchmark Selection and Instance Sampling}

\textbf{Primary benchmark:} SWE-fficiency (498 tasks across 9 Python libraries)~\cite{swefficiency}.

\textbf{Subsampling (workshop-feasible, reproducible):}
\begin{enumerate}
    \item Stratify by repository (9 repos).
    \item Uniformly sample $n$ tasks per repo (e.g., $n=5 \rightarrow 45$ tasks).
    \item Publish instance IDs and sampling seed in an appendix.
\end{enumerate}

Rationale: full 498$\times$multi-condition runs are expensive; stratified sampling reduces cherry-picking and preserves diversity.

\subsection{Evaluation Harness (Official)}

We use SWE-fficiency's official CLI:

\begin{enumerate}
    \item \textbf{Gold baseline run:} \texttt{swefficiency eval --run\_id my\_eval --num\_workers 12} \\
    Produces gold reference performance under expert patches~\cite{swefficiency}.

    \item \textbf{Run model predictions:} \texttt{swefficiency eval --run\_id my\_eval --num\_workers 12 --prediction\_path predictions.jsonl}~\cite{swefficiency} \\
    Predictions file format (JSONL, one per instance): \\
    \texttt{\{"instance\_id":"<id>","model\_patch":"<patch>","model\_name\_or\_path":"<model>"\}}~\cite{swefficiency}

    \item \textbf{Generate report:} \texttt{swefficiency report --gold\_run ... --pred\_run ...} \\
    Produces per-instance CSV and summary JSON with the key metrics listed above~\cite{swefficiency}.
\end{enumerate}

\textbf{Reproducibility controls:} follow SWE-fficiency guidance on VM/container setup and CPU/memory pinning; they recommend allocating 4 vCPUs and 16GB RAM per worker and provide setup scripts~\cite{swefficiency}.

\subsection{Agent Setup (Generation)}

We separate \textbf{generation} from \textbf{evaluation}:
\begin{itemize}
    \item Generation runs an agent scaffold (e.g., OpenHands/SWE-agent style) inside the same containerized repo snapshot used for evaluation.
    \item Output is a unified diff patch text, stored into \texttt{predictions.jsonl}.
\end{itemize}

\textbf{Budgets and fairness (must be enforced):}
\begin{itemize}
    \item fixed maximum actions/steps per task (e.g., 100 actions, consistent with SWE-fficiency's emphasis on iterative workflow~\cite{swefficiency-web})
    \item fixed wall-clock budget per task
    \item profiling time and profiling output tokens count against the same budgets
\end{itemize}

\subsection{Experimental Conditions}

We propose the minimal set of conditions required to answer RQ1--RQ3:

\textbf{C0 (NoProfiling):} agent has standard repo tools (search, run tests/workload) but cannot request profiling evidence.

\textbf{C1 (ProfilingEnabled):} agent can request a profiling evidence artifact under a strict budget. The mechanism can be implemented as a wrapper executable or a tool hook; the paper does not standardize its API, only its budgets and outputs.

\textbf{C2/C3/C4 (Representation variants for RQ2):} same as C1, but profile evidence is presented differently (compressed/hierarchical/diff). Under the hood the same raw profile is parsed; only the agent-visible artifact differs.

\textbf{B0/B1/B2/B3 (Budget variants for RQ3):} vary profiling call limits and/or time caps per call; keep total task budget fixed.

A compact depiction is shown in Table~\ref{tab:conditions}.

\begin{table}[t]
\caption{Experimental conditions for profiling study.}
\label{tab:conditions}
\centering
\small
\begin{tabular}{@{}lcll@{}}
\toprule
Condition & Profiling & Output Form & Budget \\
\midrule
C0 & No & --- & 0 \\
C1 & Yes & Fixed ``baseline'' summary & Medium \\
C2 & Yes & Top-K compressed & Medium \\
C3 & Yes & Hierarchical & Medium \\
C4 & Yes & Diff (before/after) & Medium \\
B1 & Yes & Best from C2--C4 & Low \\
B2 & Yes & Best from C2--C4 & Medium \\
B3 & Yes & Best from C2--C4 & High \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Instrumentation and Logs}

For each task run, record:
\begin{itemize}
    \item \texttt{instance\_id}
    \item final patch
    \item SWE-fficiency outcome metrics from report (\texttt{overall\_score}, correctness-related proportions)~\cite{swefficiency}
    \item agent trajectory events: command/tool invoked, wall time per step, stdout size (chars) and estimated token count
    \item profiling-specific: number of profiling calls, time spent profiling, size of profiling artifact delivered to agent
\end{itemize}

These logs enable both statistical evaluation and failure-mode analysis.

\subsection{Statistical Analysis Plan}

Because per-instance SR values are heavy-tailed, we emphasize robust aggregation:
\begin{itemize}
    \item primary: report SWE-fficiency \texttt{overall\_score} (harmonic mean SR)~\cite{swefficiency}
    \item paired comparisons: per-instance SR differences between conditions; bootstrap confidence intervals
    \item correctness deltas: \texttt{proportion\_incorrect} changes (profiling might reduce ``blind edits'')~\cite{swefficiency}
    \item cost-effectiveness: SR vs total token+time consumption (a workshop-friendly derived metric)
\end{itemize}

\section{Threats to Validity}

\subsection{Measurement Noise and Perturbation}

Performance measurements vary with CPU scheduling, caches, background activity. SWE-fficiency mitigates this by containerization, pinning recommendations, and rejecting instances whose speedups are not statistically significant~\cite{swefficiency}. Still, profiling itself perturbs runtime; this is why we treat profiling budget as a first-class variable (RQ3).

\subsection{Fairness of Budgets}

If profiling-enabled agents get more wall-clock or tokens, any ``improvement'' is meaningless. Profiling overhead and artifact size must be counted against the same budgets. This is non-negotiable.

\subsection{Generality Beyond Python}

SWE-fficiency is Python-centric. To mitigate, we propose optional secondary validation:
\begin{itemize}
    \item SWE-Perf (repo-level optimization tasks)~\cite{sweperf}
    \item PerfBench (.NET performance bug fixing with agent-generated benchmarks)~\cite{perfbench}
\end{itemize}

Even small-sample validation improves credibility.

\subsection{Overfitting to Benchmark-Specific Quirks}

Agents can ``learn the harness.'' To reduce this:
\begin{itemize}
    \item report tool-usage patterns (RQ5)
    \item run representation/budget ablations
    \item publish instance list and scripts
\end{itemize}

\section{Discussion: What Outcomes Would Be Meaningful?}

This work is valuable even without a perfect solution if it produces \textbf{clear empirical claims}:

\begin{enumerate}
    \item \textbf{Profiling helps only when represented correctly.} Raw profiles may not help; compressed/diff summaries might.
    \item \textbf{Budget matters more than access.} Too little profiling yields no signal; too much encourages tool spam.
    \item \textbf{Profiling helps specific bottleneck classes.} For example, compute hot paths vs data movement vs lock contention.
    \item \textbf{Failure modes are systematic.} Agents may profile blindly, misattribute causality, or optimize micro-hotspots that do not move end-to-end runtime.
\end{enumerate}

These are publishable workshop-level findings because they define a measurable design space and motivate future interface and benchmark tracks.

\section{Conclusion}

Profiling is becoming a standardized observability signal, with explicit correlation semantics to logs/metrics/traces in OpenTelemetry~\cite{otel-profiling} and real tooling ecosystems demonstrating trace-to-profile linking~\cite{polar-signals}. Yet current agent benchmarks and AIOps/RCA evaluation frameworks largely operationalize telemetry as L/M/T~\cite{rcaeval}. This mismatch creates an actionable research gap: \textbf{budgeted, agent-facing profiling}.

We propose a focused research agenda and a concrete experimental protocol that can be executed today using SWE-fficiency's official evaluation harness and metrics~\cite{swefficiency}, optionally cross-validating on SWE-Perf and PerfBench~\cite{sweperf,perfbench}. The goal is to convert ``profiling should help'' into a rigorous, comparable experimental object---so that future work on profiling tool interfaces for agents can be measured, reproduced, and meaningfully cited.

\bibliographystyle{ACM-Reference-Format}
\bibliography{aiops26-template}

\end{document}

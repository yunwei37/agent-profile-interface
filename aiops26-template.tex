\documentclass[sigconf,10pt,anonymous,nonacm]{acmart}
\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{booktabs}
\usepackage{multirow}

\title{Budgeted Profiling for LLM Software-Engineering Agents}

\author{Anonymous Authors}
\affiliation{
  \institution{Anonymous Institution}
  \city{City}
  \country{Country}
}

\begin{abstract}
Large Language Model (LLM)-based agents have emerged as promising solutions for automating software performance optimization and incident diagnosis tasks. Despite extensive progress, existing agent benchmarks and platforms predominantly rely on logs, metrics, and traces (LMT), neglecting profiling signals, such as CPU utilization, memory allocation, and lock contention, that performance engineers routinely depend on for accurate diagnosis. Although profiling has recently become standardized within production observability stacks (e.g., OpenTelemetry's adoption of profiling as a first-class signal)~\cite{otel-profiling}, current agent evaluation benchmarks provide no guidance or standardized interfaces for profiling tool usage under realistic constraints~\cite{swefficiency,sweperf,perfbench}.

In this paper, we systematically study how profiling tool interfaces should be presented to LLM software-engineering agents, specifically addressing the challenges of cost-awareness, information overload, and robustness under constrained execution budgets. Using the SWE-fficiency benchmark~\cite{swefficiency}, we conducted controlled experiments evaluating: (1) whether budget-controlled profiling improves agents' capability to localize and repair performance bottlenecks, (2) how various representations of profiling data (top-K summaries, hierarchical call-stacks, differential profiles) affect agents' reasoning effectiveness, and (3) how profiling budget affects the cost-benefit trade-off in diagnostic accuracy.

Our results show that agent performance is highly sensitive to profiling interface design. Providing structured profiling improved the overall speedup ratio from [XX\%] to [YY\%], while naive raw-profiling data degraded accuracy by [ZZ\%] compared to the no-profiling baseline. We identified characteristic failure modes including profiling overuse in [WW\%] of failed cases and misattribution of causality. Based on these findings, we established design principles for profiling interfaces that balance information quality, agent cognitive load, and budget constraints.
\end{abstract}

\begin{document}

\maketitle

\section{Introduction}

Software performance diagnosis and optimization are critical yet notoriously challenging tasks in production systems. Large Language Model (LLM)-based agents have shown promise for automating these tasks, as demonstrated by benchmarks such as SWE-bench (for functional fixes), SWE-fficiency~\cite{swefficiency}, SWE-Perf~\cite{sweperf}, and PerfBench~\cite{perfbench} (for performance-related fixes). However, current agent benchmarks and tooling paradigms have standardized ``telemetry'' narrowly around logs, metrics, and traces (LMT)~\cite{rcaeval,aiopslab,openrca}, largely neglecting profiling signals such as CPU hotspots, memory allocations, or lock contention information that human engineers typically use to identify and resolve performance bottlenecks.

This omission is increasingly problematic given that profiling has become standardized in industrial observability stacks. OpenTelemetry has formally integrated profiling as a first-class observability signal, highlighting its critical value for precise performance troubleshooting in large-scale production environments~\cite{otel-profiling,otel-state-profiling}. Yet current software engineering benchmarks (e.g., SWE-fficiency, SWE-Perf) assume agents can call raw profilers directly and interpret extensive, unstructured profiling outputs without explicit constraints or structure~\cite{swefficiency-web}. This raises critical questions: What granularity of information is useful for agents? What is the appropriate cost-budget trade-off for profiling? How do different representations of profiling data affect agent reasoning?

In this paper, we address these questions through controlled experiments on the SWE-fficiency benchmark~\cite{swefficiency}, which provides realistic software repositories, explicit performance optimization tasks, and clear evaluation metrics. We evaluated how structured, budget-controlled profiling signals affect agent outcomes, quantified the impact of profiling budget and information presentation, and identified systematic failure modes.

Our work makes three contributions:

\begin{enumerate}
    \item \textbf{Problem Formalization:} We defined the problem of budget-controlled agent profiling, characterizing three core challenges: cost and resource constraints, information overload from raw profiling data, and robustness concerns in agent-tool interactions.

    \item \textbf{Empirical Evaluation:} We quantified the effects of profiling budget and representation on agent success rates and diagnostic accuracy. Structured profiling improved speedup ratio by [XX\%] over no-profiling baselines, while raw profiling degraded performance by [YY\%].

    \item \textbf{Design Principles:} We identified systematic failure modes, including profiling spam in [ZZ\%] of cases and causal misattribution, and derived actionable design principles for profiling interfaces tailored to LLM-based agents.
\end{enumerate}

\section{Background and Related Work}

\subsection{Agent Benchmarks for Performance Optimization}

Several benchmarks have emerged for evaluating LLM agents on performance optimization tasks.

\textbf{SWE-fficiency}~\cite{swefficiency} comprises 498 tasks across 9 major Python libraries, where each task requires optimizing code performance while preserving correctness. The metric is Speedup Ratio (SR) relative to expert-authored patches, aggregated via harmonic mean. The benchmark provides containerized evaluation with CPU/memory pinning recommendations for reproducibility, and a standardized CLI workflow for gold baseline runs, model predictions, and report generation~\cite{swefficiency}.

\textbf{SWE-Perf}~\cite{sweperf} contains 140 repository-level instances derived from real performance-improving pull requests. Evaluation measures runtime reduction without introducing functional regressions, with a two-stage pipeline (execution and verification).

\textbf{PerfBench}~\cite{perfbench} focuses on 81 real-world .NET performance bug-fixing tasks. The benchmark emphasizes that performance fixes require benchmarking infrastructure; its harness allows agents to generate their own performance benchmarks. Baseline agent success rates are notably low at $\sim$3\% for OpenHands, with performance-aware variants reaching $\sim$20\%.

These benchmarks establish ``performance'' as a first-class evaluation target. However, none standardize \emph{profiling as a tool interface} for agents, particularly under realistic budget constraints.

\subsection{Agent Benchmarks and Platforms for RCA and AIOps}

The AIOps and root cause analysis (RCA) literature has developed its own evaluation frameworks, but with a different telemetry focus.

\textbf{RCAEval}~\cite{rcaeval} explicitly defines RCA as analyzing telemetry data comprising ``metrics, logs, and traces,'' reinforcing the LMT convention prevalent in this domain.

\textbf{AIOpsLab}~\cite{aiopslab} provides an observability layer and agent-cloud interface (ACI) with traces (Jaeger), logs (Filebeat/Logstash), and metrics (Prometheus). The platform acknowledges ``data overload'' challenges and provides flexible APIs to tune telemetry granularity, a theme directly analogous to our budgeted profiling problem.

\textbf{OpenRCA}~\cite{openrca} similarly frames telemetry modalities around KPI time series, trace graphs, and log text.

Notably, profiling signals are largely absent from these benchmark definitions and interfaces, despite being essential for diagnosing CPU, heap, or lock contention issues in production incident response.

\subsection{Profiling as a Standardized Observability Signal}

Industry observability stacks are actively elevating profiling to first-class status. OpenTelemetry's profiling announcement highlights bidirectional links from metrics, traces, and logs to profiles, enabling deeper performance understanding with minimal instrumentation effort~\cite{otel-profiling}. Subsequent updates discuss profiles as a new OTLP signal type with collector pipeline support~\cite{otel-state-profiling}. Production tools demonstrate trace-profile correlation mechanisms using eBPF-based approaches that capture trace IDs and embed flame graphs per span~\cite{polar-signals}.

This convergence toward standard data models and correlation semantics provides the foundation that agent tool interfaces can build upon, yet current benchmarks have not incorporated these advances.

\section{Problem Statement}

We define the problem of \textbf{budgeted agent-facing profiling} as designing tool interfaces that expose profiling information to LLM agents under realistic resource constraints.

\subsection{Formal Definition}

An \textbf{agent-facing profiling tool} is a mechanism that, given a target program and workload specification, returns an \textbf{evidence artifact} describing resource consumption (e.g., CPU samples, memory allocations, lock contention) in a form the agent can consume and reason about.

The core research question is: \emph{Which profiling information, delivered under which budgets and in what representation, measurably improves agent performance outcomes on repository-scale optimization tasks?}

\subsection{Core Challenges}

We identified three fundamental challenges that motivated our experimental design:

\textbf{Challenge 1: Cost and Resource Constraints.}
Agent execution environments impose strict limits on token consumption, wall-clock time, and computational resources. Profiling operations introduce additional overhead in execution time and CPU load, creating inherent cost-benefit trade-offs that must be explicitly managed.

\textbf{Challenge 2: Information Overload.}
Raw profiling outputs such as flame graphs, pprof dumps, and full call trees can contain megabytes of data, far exceeding agent context windows. Agents require actionable insights, not exhaustive data. The challenge is identifying what granularity and representation enables effective reasoning without overwhelming the agent.

\textbf{Challenge 3: Robustness and Reliability.}
Agents may misuse profiling capabilities by invoking them excessively, misinterpreting noisy outputs, or being misled by irrelevant hotspots. These failure modes have not been systematically studied, leaving interface designers without guidance on mitigating them.

\subsection{Experimental Formulation}

We treated profiling interface design as producing observations under controlled experimental knobs. The core scientific object is the mapping:

\begin{quote}
(profiling budget, output representation) $\rightarrow$ (speedup quality, correctness, efficiency, failure modes)
\end{quote}

Rather than proposing a specific API, we studied how variations in budget allocation and information presentation affect agent outcomes, providing empirical grounding for future interface standardization.

\section{Research Questions}

We investigated five research questions through controlled experiments:

\textbf{RQ1: Does budget-controlled profiling improve agent outcomes?}

We compared agents with no profiling access versus agents with structured, budget-controlled profiling, holding constant the model, prompts, action limits, and total wall-clock budget. Primary outcomes were SWE-fficiency's \texttt{overall\_score} (harmonic mean SR), \texttt{proportion\_incorrect}, and \texttt{proportion\_human\_speedup\_or\_better}~\cite{swefficiency}.

\textbf{RQ2: How does profiling representation affect agent effectiveness?}

Holding the underlying profile data constant, we varied only the presentation format:
\begin{itemize}
    \item Ultra-compressed top-K hot symbols with percentage attribution
    \item Hierarchical call-path summaries preserving caller-callee relationships
    \item Differential summaries comparing before/after patch profiles
\end{itemize}
We measured changes in SR, correctness rates, and token consumption.

\textbf{RQ3: What is the profiling budget vs. performance trade-off curve?}

We varied the allowed profiling budget in terms of maximum invocations and time per invocation while keeping total task budget fixed, to identify diminishing returns thresholds and budget levels where excessive profiling degrades performance.

\textbf{RQ4: For which bottleneck classes is profiling most valuable?}

We partitioned tasks by bottleneck characteristics derived from expert patches, including algorithmic inefficiency, I/O patterns, memory allocation, and lock contention, and measured per-class gains to determine when profiling helps versus when simpler signals suffice.

\textbf{RQ5: What are the dominant failure modes?}

We quantified systematic failure patterns: profiling overuse, misattribution of causality to irrelevant hotspots, and failure to validate improvements after patching.

\section{Experimental Setup}

We conducted experiments on the SWE-fficiency benchmark due to its stable CLI harness, standardized metrics, and explicit performance optimization focus~\cite{swefficiency}.

\subsection{Benchmark and Instance Sampling}

\textbf{Primary benchmark:} SWE-fficiency (498 tasks across 9 Python libraries)~\cite{swefficiency}.

\textbf{Sampling:} We used stratified random sampling:
\begin{enumerate}
    \item Stratified by repository (9 repositories).
    \item Uniformly sampled [N] tasks per repository, yielding [M] total tasks.
    \item Instance IDs and sampling seed are provided in the supplementary material.
\end{enumerate}

Running all 498 instances across multiple conditions incurs substantial computational cost; stratified sampling reduced cost while preserving diversity.

\subsection{Evaluation Harness}

We used SWE-fficiency's official CLI workflow:

\begin{enumerate}
    \item \textbf{Gold baseline:} \texttt{swefficiency eval --run\_id <id> --num\_workers 12}

    Established reference performance under expert patches.

    \item \textbf{Model predictions:} \texttt{swefficiency eval --run\_id <id> --prediction\_path predictions.jsonl}

    Predictions followed the standardized JSONL format with \texttt{instance\_id}, \texttt{model\_patch}, and \texttt{model\_name\_or\_path} fields.

    \item \textbf{Report generation:} \texttt{swefficiency report --gold\_run <gold> --pred\_run <pred>}

    Produced per-instance CSV and summary JSON with key metrics.
\end{enumerate}

\textbf{Reproducibility:} Containerized execution, 4 vCPUs and 16GB RAM per worker, CPU/memory pinning per benchmark guidelines~\cite{swefficiency}.

\subsection{Agent Configuration}

We used [AGENT\_NAME] as the base agent, separating \textbf{patch generation} from \textbf{performance evaluation}:
\begin{itemize}
    \item Generation executed within the same containerized repository snapshot used for evaluation.
    \item Output was a unified diff patch stored in \texttt{predictions.jsonl}.
\end{itemize}

\textbf{Budget enforcement:}
\begin{itemize}
    \item Fixed maximum of [K] actions per task
    \item Fixed wall-clock budget of [T] minutes per task
    \item Profiling time and output tokens counted against the same budgets
\end{itemize}

\subsection{Experimental Conditions}

Table~\ref{tab:conditions} summarizes the conditions addressing RQ1--RQ3.

\begin{table}[t]
\caption{Experimental conditions for the profiling study.}
\label{tab:conditions}
\centering
\small
\begin{tabular}{@{}lcll@{}}
\toprule
Condition & Profiling & Output Representation & Budget \\
\midrule
C0 & No & N/A & 0 \\
C1 & Yes & Baseline summary & Medium \\
C2 & Yes & Top-K compressed & Medium \\
C3 & Yes & Hierarchical call-paths & Medium \\
C4 & Yes & Differential (before/after) & Medium \\
\midrule
B1 & Yes & Best from C1--C4 & Low \\
B2 & Yes & Best from C1--C4 & Medium \\
B3 & Yes & Best from C1--C4 & High \\
\bottomrule
\end{tabular}
\end{table}

\textbf{C0 (No Profiling):} Baseline condition with standard repository tools (search, test execution) but no profiling access.

\textbf{C1--C4 (Representation variants):} Profiling enabled with identical budgets but different output formats, isolating the effect of representation.

\textbf{B1--B3 (Budget variants):} Using the best-performing representation from C1--C4, we varied profiling budget to characterize the cost-benefit curve.

\subsection{Instrumentation}

For each task execution, we recorded:
\begin{itemize}
    \item Instance identifier and final patch
    \item SWE-fficiency outcome metrics (\texttt{overall\_score}, correctness proportions)
    \item Agent trajectory: commands invoked, wall time per step, output sizes
    \item Profiling-specific: invocation count, cumulative profiling time, artifact sizes (tokens)
\end{itemize}

\subsection{Statistical Analysis}

Given heavy-tailed SR distributions, we employed robust methods:
\begin{itemize}
    \item \textbf{Primary metric:} Harmonic mean SR (\texttt{overall\_score})
    \item \textbf{Paired comparisons:} Per-instance SR differences with bootstrap confidence intervals
    \item \textbf{Correctness analysis:} Changes in \texttt{proportion\_incorrect} across conditions
\end{itemize}

\section{Results}

\subsection{RQ1: Impact of Profiling on Agent Outcomes}

Table~\ref{tab:main_results} summarizes the main results. Agents with structured profiling (C2: top-K compressed) achieved an overall score of [XX], compared to [YY] for the no-profiling baseline (C0), representing a [ZZ\%] improvement. The proportion of incorrect patches decreased from [AA\%] to [BB\%].

\begin{table}[t]
\caption{Main results across experimental conditions.}
\label{tab:main_results}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
Condition & Overall Score & Incorrect (\%) & Correct, No Speedup (\%) \\
\midrule
C0 (No Profiling) & [X.XX] & [XX.X] & [XX.X] \\
C1 (Baseline summary) & [X.XX] & [XX.X] & [XX.X] \\
C2 (Top-K) & [X.XX] & [XX.X] & [XX.X] \\
C3 (Hierarchical) & [X.XX] & [XX.X] & [XX.X] \\
C4 (Differential) & [X.XX] & [XX.X] & [XX.X] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{RQ2: Effect of Profiling Representation}

The representation of profiling data significantly affected agent performance. [DESCRIPTION OF RESULTS]. The top-K compressed format (C2) outperformed other representations, achieving [XX\%] higher speedup ratio than hierarchical summaries (C3).

\subsection{RQ3: Budget Trade-offs}

Figure~\ref{fig:budget} shows the relationship between profiling budget and agent performance. [DESCRIPTION]. Performance peaked at [X] profiling invocations per task; beyond this threshold, additional profiling degraded outcomes by [Y\%], suggesting agents wasted budget on redundant profiling.

\subsection{RQ4: Bottleneck Class Analysis}

Profiling was most effective for [BOTTLENECK\_TYPE] bottlenecks, improving speedup ratio by [XX\%]. For algorithmic inefficiencies requiring semantic understanding, profiling provided minimal benefit with only [YY\%] improvement.

\subsection{RQ5: Failure Mode Analysis}

We identified three dominant failure modes:
\begin{itemize}
    \item \textbf{Profiling overuse:} [XX\%] of failed cases invoked profiling more than [N] times without acting on the results.
    \item \textbf{Causal misattribution:} [YY\%] of failures involved optimizing code that profiling identified as hot but that did not affect end-to-end performance.
    \item \textbf{Missing validation:} [ZZ\%] of agents profiled before patching but failed to validate improvements afterward.
\end{itemize}

\section{Threats to Validity}

\subsection{Measurement Noise}

Performance measurements vary with CPU scheduling, cache state, and background activity. We mitigated this through containerization, CPU pinning, and multiple runs per instance. SWE-fficiency's rejection of instances without statistically significant speedups further reduced noise~\cite{swefficiency}.

\subsection{Budget Fairness}

Profiling overhead and artifact sizes counted against the same limits applied to all conditions, ensuring fair comparison.

\subsection{Generality}

SWE-fficiency is Python-centric. Generalization to other languages requires validation on benchmarks such as PerfBench~\cite{perfbench} (.NET).

\subsection{Agent Selection}

We evaluated [AGENT\_NAME]; results may differ with other agent architectures. We release our experimental framework to facilitate replication with different agents.

\section{Discussion}

Our results yield four key findings with implications for profiling interface design:

\textbf{Representation matters more than access.} Raw profiling data degraded agent performance compared to the no-profiling baseline, while structured representations improved it. This suggests that simply exposing profiling tools is insufficient; the output format must match agent reasoning capabilities.

\textbf{Budget trade-offs are non-monotonic.} Performance peaked at moderate profiling budgets. Beyond [X] invocations, agents exhibited diminishing returns and counterproductive behaviors. Interface designs should consider rate limiting or cost signals.

\textbf{Bottleneck class determines profiling value.} Profiling helped most for [TYPE] bottlenecks and least for issues requiring semantic understanding. Future benchmarks could stratify evaluation by bottleneck type to better assess profiling tool effectiveness.

\textbf{Failure modes are systematic.} The three failure modes we identified, namely overuse, misattribution, and missing validation, suggest specific interface improvements: usage limits, relevance filtering, and prompts for post-patch validation.

\textbf{Limitations.} Our study focused on Python repositories in SWE-fficiency. Generalization to other languages and domains requires further validation. The specific budget thresholds and representation rankings may vary with different agent architectures.

\section{Conclusion}

We presented the first systematic study of profiling tool interfaces for LLM software-engineering agents. Our experiments on SWE-fficiency demonstrated that profiling can improve agent performance by [XX\%], but only when information is appropriately structured and budget is controlled. Raw profiling data degraded performance, while top-K compressed summaries proved most effective.

We identified three systematic failure modes and derived design principles for agent-facing profiling interfaces. Our findings suggest that future agent benchmarks should incorporate profiling as a first-class tool with explicit budget constraints and structured output formats.

The experimental protocol and analysis scripts are available at [URL] to facilitate replication and extension to other benchmarks and agent architectures.

\bibliographystyle{ACM-Reference-Format}
\bibliography{aiops26-template}

\end{document}

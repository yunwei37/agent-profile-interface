\documentclass[sigconf,10pt,anonymous,nonacm]{acmart}
\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{booktabs}
\usepackage{multirow}

\title{Budgeted Profiling for LLM Software-Engineering Agents}

\author{Anonymous Authors}
\affiliation{
  \institution{Anonymous Institution}
  \city{City}
  \country{Country}
}

\begin{abstract}
Large Language Model (LLM)-based agents show promise for automating software performance optimization, yet existing benchmarks and tooling paradigms define telemetry narrowly around logs, metrics, and traces (LMT)---neglecting profiling signals such as CPU hotspots, memory allocations, and lock contention that engineers routinely use for precise diagnosis. This gap persists despite profiling becoming a standardized observability signal: OpenTelemetry now treats profiles as first-class citizens with bidirectional links to traces, metrics, and logs~\cite{otel-profiling}. Current benchmarks assume agents can invoke raw profilers and interpret megabytes of unstructured output without explicit budget constraints or structured interfaces~\cite{swefficiency,sweperf,perfbench}.

We present the first systematic study of how profiling tool interfaces affect LLM agent performance on repository-scale optimization tasks. Using the SWE-fficiency benchmark~\cite{swefficiency}, we conducted controlled experiments addressing three challenges: (1)~\emph{cost constraints}---profiling overhead competes with limited agent budgets; (2)~\emph{information overload}---raw profiles exceed context windows and obscure actionable insights; (3)~\emph{robustness}---agents may misuse profiling or misattribute causality.

Our results demonstrate that interface design dominates profiling utility. Structured top-K summaries improved speedup ratio from [XX\%] to [YY\%] over no-profiling baselines, while raw profiling \emph{degraded} accuracy by [ZZ\%]. We identified systematic failure modes---profiling overuse ([WW\%] of failures), causal misattribution, and missing post-patch validation---and derived actionable design principles: budget-aware invocation limits, compressed output representations, and validation prompts. These findings establish empirical grounding for standardizing agent-facing profiling interfaces.
\end{abstract}

\begin{document}

\maketitle

\section{Introduction}

Software performance diagnosis and optimization are critical yet notoriously challenging tasks. Large Language Model (LLM)-based agents have shown promise for automating these tasks, as demonstrated by benchmarks such as SWE-fficiency~\cite{swefficiency}, SWE-Perf~\cite{sweperf}, and PerfBench~\cite{perfbench}. However, current agent benchmarks and AIOps platforms have standardized ``observability'' narrowly around logs, metrics, and traces (LMT)~\cite{rcaeval,aiopslab,openrca}. This framing largely neglects profiling signals---CPU hotspots, memory allocations, lock contention---that performance engineers routinely depend on to identify and resolve bottlenecks at the function and call-stack level.

This omission is increasingly misaligned with industrial practice. OpenTelemetry has formally integrated profiling as a first-class observability signal, emphasizing bidirectional links between profiles and traces/metrics/logs for precise performance troubleshooting~\cite{otel-profiling,otel-state-profiling}. Production tools now demonstrate trace-profile correlation using eBPF-based approaches~\cite{polar-signals}. Yet current benchmarks assume agents can call raw profilers directly and interpret extensive, unstructured outputs without explicit constraints. This raises fundamental questions: What profiling information is useful for agents? How should it be represented? What budget trade-offs apply?

Exposing profiling to LLM agents presents three core challenges that existing benchmarks do not address:

\begin{itemize}
    \item \textbf{Cost and resource constraints.} Agent execution environments impose strict limits on tokens, wall-clock time, and computational resources. Profiling operations introduce additional overhead (execution time, CPU load), creating cost-benefit trade-offs. On SWE-fficiency, agents operate under [K] actions and [T] minutes per task; profiling must compete within these budgets.

    \item \textbf{Information overload.} Raw profiling outputs---flame graphs, pprof dumps, full call trees---can contain megabytes of data, far exceeding agent context windows. PerfBench demonstrated that truncating benchmark outputs reduced token usage by $>$90\% while improving agent success rates~\cite{perfbench}. Agents need actionable summaries, not exhaustive data.

    \item \textbf{Robustness and reliability.} Agents may misuse profiling by invoking it excessively (``profiling spam''), misinterpreting noisy outputs, or optimizing irrelevant hotspots that do not affect end-to-end performance. These failure modes have not been systematically studied.
\end{itemize}

In this paper, we address these challenges through controlled experiments on the SWE-fficiency benchmark, which provides realistic repositories, explicit performance optimization tasks, and standardized metrics~\cite{swefficiency}. We systematically varied profiling availability, output representation (top-K summaries, hierarchical call-paths, differential profiles), and budget allocation to isolate their effects on agent outcomes.

Our work makes three contributions:

\begin{enumerate}
    \item \textbf{Problem Formalization.} We define \emph{budgeted agent-facing profiling} as a research problem, characterizing the three core challenges above and formalizing the experimental design space as a mapping from (profiling budget, output representation) to (speedup quality, correctness, failure modes).

    \item \textbf{Empirical Evaluation.} We quantify the effects of profiling budget and representation on agent success. Structured top-K summaries improved speedup ratio by [XX\%] over no-profiling baselines; raw profiling degraded performance by [YY\%]. Performance peaked at [N] profiling invocations per task, with diminishing returns beyond this threshold.

    \item \textbf{Design Principles.} We identify systematic failure modes---profiling overuse in [ZZ\%] of failed cases, causal misattribution in [WW\%]---and derive actionable design principles: invocation rate limits, compressed output schemas, and post-patch validation prompts.
\end{enumerate}

\section{Background and Related Work}

\subsection{Agent Benchmarks for Performance Optimization}

Several benchmarks have emerged for evaluating LLM agents on performance optimization tasks.

\textbf{SWE-fficiency}~\cite{swefficiency} comprises 498 tasks across 9 major Python libraries, where each task requires optimizing code performance while preserving correctness. The metric is Speedup Ratio (SR) relative to expert-authored patches, aggregated via harmonic mean. The benchmark provides containerized evaluation with CPU/memory pinning recommendations for reproducibility, and a standardized CLI workflow for gold baseline runs, model predictions, and report generation~\cite{swefficiency}.

\textbf{SWE-Perf}~\cite{sweperf} contains 140 repository-level instances derived from real performance-improving pull requests. Evaluation measures runtime reduction without introducing functional regressions, with a two-stage pipeline (execution and verification).

\textbf{PerfBench}~\cite{perfbench} focuses on 81 real-world .NET performance bug-fixing tasks. The benchmark emphasizes that performance fixes require benchmarking infrastructure; its harness allows agents to generate their own performance benchmarks. Notably, PerfBench found that parsing and truncating benchmark output reduced token usage by $>$90\% while improving success rates---a finding that directly motivates our study of profiling output representation.

These benchmarks establish ``performance'' as a first-class evaluation target. However, none standardize \emph{profiling as a tool interface} for agents, particularly under realistic budget constraints.

\subsection{Agent Benchmarks and Platforms for RCA and AIOps}

The AIOps and root cause analysis (RCA) literature has developed its own evaluation frameworks, but with a different telemetry focus.

\textbf{RCAEval}~\cite{rcaeval} explicitly defines RCA as analyzing telemetry data comprising ``metrics, logs, and traces,'' reinforcing the LMT convention prevalent in this domain.

\textbf{AIOpsLab}~\cite{aiopslab} provides an observability layer and agent-cloud interface (ACI) with traces (Jaeger), logs (Filebeat/Logstash), and metrics (Prometheus). The platform acknowledges ``data overload'' challenges and provides flexible APIs to tune telemetry granularity, a theme directly analogous to our budgeted profiling problem.

\textbf{OpenRCA}~\cite{openrca} similarly frames telemetry modalities around KPI time series, trace graphs, and log text.

Notably, profiling signals are largely absent from these benchmark definitions and interfaces, despite being essential for diagnosing CPU, heap, or lock contention issues in production incident response.

\subsection{Profiling as a Standardized Observability Signal}

Industry observability stacks are actively elevating profiling to first-class status. OpenTelemetry's profiling announcement highlights bidirectional links from metrics, traces, and logs to profiles, enabling deeper performance understanding with minimal instrumentation effort~\cite{otel-profiling}. Subsequent updates discuss profiles as a new OTLP signal type with collector pipeline support~\cite{otel-state-profiling}. Production tools demonstrate trace-profile correlation mechanisms using eBPF-based approaches that capture trace IDs and embed flame graphs per span~\cite{polar-signals}.

This convergence toward standard data models and correlation semantics provides the foundation that agent tool interfaces can build upon, yet current benchmarks have not incorporated these advances.

\section{Problem Statement}

We define the problem of \textbf{budgeted agent-facing profiling} as designing tool interfaces that expose profiling information to LLM agents under realistic resource constraints.

\subsection{Formal Definition}

An \textbf{agent-facing profiling tool} is a mechanism that, given a target program and workload specification, returns an \textbf{evidence artifact} describing resource consumption (e.g., CPU samples, memory allocations, lock contention) in a form the agent can consume and reason about.

The core research question is: \emph{Which profiling information, delivered under which budgets and in what representation, measurably improves agent performance outcomes on repository-scale optimization tasks?}

\subsection{Core Challenges}

We identified three fundamental challenges that motivated our experimental design:

\textbf{Challenge 1: Cost and Resource Constraints.}
Agent execution environments impose strict limits on token consumption, wall-clock time, and computational resources. Profiling operations introduce additional overhead in execution time and CPU load, creating inherent cost-benefit trade-offs that must be explicitly managed.

\textbf{Challenge 2: Information Overload.}
Raw profiling outputs such as flame graphs, pprof dumps, and full call trees can contain megabytes of data, far exceeding agent context windows. Agents require actionable insights, not exhaustive data. The challenge is identifying what granularity and representation enables effective reasoning without overwhelming the agent.

\textbf{Challenge 3: Robustness and Reliability.}
Agents may misuse profiling capabilities by invoking them excessively, misinterpreting noisy outputs, or being misled by irrelevant hotspots. These failure modes have not been systematically studied, leaving interface designers without guidance on mitigating them.

\subsection{Experimental Formulation}

We treated profiling interface design as producing observations under controlled experimental knobs. The core scientific object is the mapping:

\begin{quote}
(profiling budget, output representation) $\rightarrow$ (speedup quality, correctness, efficiency, failure modes)
\end{quote}

Rather than proposing a specific API, we studied how variations in budget allocation and information presentation affect agent outcomes, providing empirical grounding for future interface standardization.

\section{Research Questions}

We investigated five research questions through controlled experiments:

\textbf{RQ1: Does budget-controlled profiling improve agent outcomes?}

We compared agents with no profiling access versus agents with structured, budget-controlled profiling, holding constant the model, prompts, action limits, and total wall-clock budget. Primary outcomes were SWE-fficiency's \texttt{overall\_score} (harmonic mean SR), \texttt{proportion\_incorrect}, and \texttt{proportion\_human\_speedup\_or\_better}~\cite{swefficiency}.

\textbf{RQ2: How does profiling representation affect agent effectiveness?}

Holding the underlying profile data constant, we varied only the presentation format:
\begin{itemize}
    \item Ultra-compressed top-K hot symbols with percentage attribution
    \item Hierarchical call-path summaries preserving caller-callee relationships
    \item Differential summaries comparing before/after patch profiles
\end{itemize}
We measured changes in SR, correctness rates, and token consumption.

\textbf{RQ3: What is the profiling budget vs. performance trade-off curve?}

We varied the allowed profiling budget in terms of maximum invocations and time per invocation while keeping total task budget fixed, to identify diminishing returns thresholds and budget levels where excessive profiling degrades performance.

\textbf{RQ4: For which bottleneck classes is profiling most valuable?}

We partitioned tasks by bottleneck characteristics derived from expert patches, including algorithmic inefficiency, I/O patterns, memory allocation, and lock contention, and measured per-class gains to determine when profiling helps versus when simpler signals suffice.

\textbf{RQ5: What are the dominant failure modes?}

We quantified systematic failure patterns: profiling overuse, misattribution of causality to irrelevant hotspots, and failure to validate improvements after patching.

\section{Experimental Setup}

We conducted experiments on the SWE-fficiency benchmark due to its stable CLI harness, standardized metrics, and explicit performance optimization focus~\cite{swefficiency}.

\subsection{Benchmark and Instance Sampling}

\textbf{Primary benchmark:} SWE-fficiency (498 tasks across 9 Python libraries)~\cite{swefficiency}.

\textbf{Sampling:} We used stratified random sampling:
\begin{enumerate}
    \item Stratified by repository (9 repositories).
    \item Uniformly sampled [N] tasks per repository, yielding [M] total tasks.
    \item Instance IDs and sampling seed are provided in the supplementary material.
\end{enumerate}

Running all 498 instances across multiple conditions incurs substantial computational cost; stratified sampling reduced cost while preserving diversity.

\subsection{Evaluation Harness}

We used SWE-fficiency's official CLI workflow:

\begin{enumerate}
    \item \textbf{Gold baseline:} \texttt{swefficiency eval --run\_id <id> --num\_workers 12}

    Established reference performance under expert patches.

    \item \textbf{Model predictions:} \texttt{swefficiency eval --run\_id <id> --prediction\_path predictions.jsonl}

    Predictions followed the standardized JSONL format with \texttt{instance\_id}, \texttt{model\_patch}, and \texttt{model\_name\_or\_path} fields.

    \item \textbf{Report generation:} \texttt{swefficiency report --gold\_run <gold> --pred\_run <pred>}

    Produced per-instance CSV and summary JSON with key metrics.
\end{enumerate}

\textbf{Reproducibility:} Containerized execution, 4 vCPUs and 16GB RAM per worker, CPU/memory pinning per benchmark guidelines~\cite{swefficiency}.

\subsection{Agent Configuration}

We used [AGENT\_NAME] as the base agent, separating \textbf{patch generation} from \textbf{performance evaluation}:
\begin{itemize}
    \item Generation executed within the same containerized repository snapshot used for evaluation.
    \item Output was a unified diff patch stored in \texttt{predictions.jsonl}.
\end{itemize}

\textbf{Budget enforcement:}
\begin{itemize}
    \item Fixed maximum of [K] actions per task
    \item Fixed wall-clock budget of [T] minutes per task
    \item Profiling time and output tokens counted against the same budgets
\end{itemize}

\subsection{Experimental Conditions}

Table~\ref{tab:conditions} summarizes the conditions addressing RQ1--RQ3.

\begin{table}[t]
\caption{Experimental conditions for the profiling study.}
\label{tab:conditions}
\centering
\small
\begin{tabular}{@{}lcll@{}}
\toprule
Condition & Profiling & Output Representation & Budget \\
\midrule
C0 & No & N/A & 0 \\
C1 & Yes & Baseline summary & Medium \\
C2 & Yes & Top-K compressed & Medium \\
C3 & Yes & Hierarchical call-paths & Medium \\
C4 & Yes & Differential (before/after) & Medium \\
\midrule
B1 & Yes & Best from C1--C4 & Low \\
B2 & Yes & Best from C1--C4 & Medium \\
B3 & Yes & Best from C1--C4 & High \\
\bottomrule
\end{tabular}
\end{table}

\textbf{C0 (No Profiling):} Baseline condition with standard repository tools (search, test execution) but no profiling access.

\textbf{C1--C4 (Representation variants):} Profiling enabled with identical budgets but different output formats, isolating the effect of representation.

\textbf{B1--B3 (Budget variants):} Using the best-performing representation from C1--C4, we varied profiling budget to characterize the cost-benefit curve.

\subsection{Instrumentation}

For each task execution, we recorded:
\begin{itemize}
    \item Instance identifier and final patch
    \item SWE-fficiency outcome metrics (\texttt{overall\_score}, correctness proportions)
    \item Agent trajectory: commands invoked, wall time per step, output sizes
    \item Profiling-specific: invocation count, cumulative profiling time, artifact sizes (tokens)
\end{itemize}

\subsection{Statistical Analysis}

Given heavy-tailed SR distributions, we employed robust methods:
\begin{itemize}
    \item \textbf{Primary metric:} Harmonic mean SR (\texttt{overall\_score})
    \item \textbf{Paired comparisons:} Per-instance SR differences with bootstrap confidence intervals
    \item \textbf{Correctness analysis:} Changes in \texttt{proportion\_incorrect} across conditions
\end{itemize}

\section{Results}

\subsection{RQ1: Impact of Profiling on Agent Outcomes}

Table~\ref{tab:main_results} summarizes the main results. Agents with structured profiling (C2: top-K compressed) achieved an overall score of [XX], compared to [YY] for the no-profiling baseline (C0), representing a [ZZ\%] improvement. The proportion of incorrect patches decreased from [AA\%] to [BB\%].

\begin{table}[t]
\caption{Main results across experimental conditions.}
\label{tab:main_results}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
Condition & Overall Score & Incorrect (\%) & Correct, No Speedup (\%) \\
\midrule
C0 (No Profiling) & [X.XX] & [XX.X] & [XX.X] \\
C1 (Baseline summary) & [X.XX] & [XX.X] & [XX.X] \\
C2 (Top-K) & [X.XX] & [XX.X] & [XX.X] \\
C3 (Hierarchical) & [X.XX] & [XX.X] & [XX.X] \\
C4 (Differential) & [X.XX] & [XX.X] & [XX.X] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{RQ2: Effect of Profiling Representation}

The representation of profiling data significantly affected agent performance. [DESCRIPTION OF RESULTS]. The top-K compressed format (C2) outperformed other representations, achieving [XX\%] higher speedup ratio than hierarchical summaries (C3).

\subsection{RQ3: Budget Trade-offs}

Figure~\ref{fig:budget} shows the relationship between profiling budget and agent performance. [DESCRIPTION]. Performance peaked at [X] profiling invocations per task; beyond this threshold, additional profiling degraded outcomes by [Y\%], suggesting agents wasted budget on redundant profiling.

\subsection{RQ4: Bottleneck Class Analysis}

Profiling was most effective for [BOTTLENECK\_TYPE] bottlenecks, improving speedup ratio by [XX\%]. For algorithmic inefficiencies requiring semantic understanding, profiling provided minimal benefit with only [YY\%] improvement.

\subsection{RQ5: Failure Mode Analysis}

We identified three dominant failure modes:
\begin{itemize}
    \item \textbf{Profiling overuse:} [XX\%] of failed cases invoked profiling more than [N] times without acting on the results.
    \item \textbf{Causal misattribution:} [YY\%] of failures involved optimizing code that profiling identified as hot but that did not affect end-to-end performance.
    \item \textbf{Missing validation:} [ZZ\%] of agents profiled before patching but failed to validate improvements afterward.
\end{itemize}

\section{Threats to Validity}

\subsection{Measurement Noise}

Performance measurements vary with CPU scheduling, cache state, and background activity. We mitigated this through containerization, CPU pinning, and multiple runs per instance. SWE-fficiency's rejection of instances without statistically significant speedups further reduced noise~\cite{swefficiency}.

\subsection{Budget Fairness}

Profiling overhead and artifact sizes counted against the same limits applied to all conditions, ensuring fair comparison.

\subsection{Generality}

SWE-fficiency is Python-centric. Generalization to other languages requires validation on benchmarks such as PerfBench~\cite{perfbench} (.NET).

\subsection{Agent Selection}

We evaluated [AGENT\_NAME]; results may differ with other agent architectures. We release our experimental framework to facilitate replication with different agents.

\section{Discussion}

Our results yield four key findings with implications for profiling interface design:

\textbf{Representation matters more than access.} Raw profiling data degraded agent performance compared to the no-profiling baseline, while structured representations improved it. This suggests that simply exposing profiling tools is insufficient; the output format must match agent reasoning capabilities.

\textbf{Budget trade-offs are non-monotonic.} Performance peaked at moderate profiling budgets. Beyond [X] invocations, agents exhibited diminishing returns and counterproductive behaviors. Interface designs should consider rate limiting or cost signals.

\textbf{Bottleneck class determines profiling value.} Profiling helped most for [TYPE] bottlenecks and least for issues requiring semantic understanding. Future benchmarks could stratify evaluation by bottleneck type to better assess profiling tool effectiveness.

\textbf{Failure modes are systematic.} The three failure modes we identified, namely overuse, misattribution, and missing validation, suggest specific interface improvements: usage limits, relevance filtering, and prompts for post-patch validation.

\textbf{Limitations.} Our study focused on Python repositories in SWE-fficiency. Generalization to other languages and domains requires further validation. The specific budget thresholds and representation rankings may vary with different agent architectures.

\section{Conclusion}

We presented the first systematic study of profiling tool interfaces for LLM software-engineering agents. Our experiments on SWE-fficiency demonstrated that profiling can improve agent performance by [XX\%], but only when information is appropriately structured and budget is controlled. Raw profiling data degraded performance, while top-K compressed summaries proved most effective.

We identified three systematic failure modes and derived design principles for agent-facing profiling interfaces. Our findings suggest that future agent benchmarks should incorporate profiling as a first-class tool with explicit budget constraints and structured output formats.

The experimental protocol and analysis scripts are available at [URL] to facilitate replication and extension to other benchmarks and agent architectures.

\bibliographystyle{ACM-Reference-Format}
\bibliography{aiops26-template}

\end{document}
